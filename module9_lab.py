{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 9 Lab: Deploying and Monitoring a Sentiment Analysis Model\n",
    "\n",
    "**Objective**: Deploy a sentiment analysis model locally and monitor performance with mock metrics.\n",
    "\n",
    "**Instructions**:\n",
    "1. Install `transformers`, `flask`, and `numpy`.\n",
    "2. Run the code to deploy and test the model.\n",
    "3. Share results in the Teachable forum.\n",
    "\n",
    "**Inspired by**: *AI Engineering* by Chip Huyen (O’Reilly, 2025), on deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install transformers flask numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from transformers import pipeline\n",
    "from flask import Flask, request, jsonify\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Initialize sentiment analysis model\n",
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "sentiment_pipeline = pipeline('sentiment-analysis', model=model_name)\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Mock monitoring metrics\n",
    "request_times = []\n",
    "accuracies = []\n",
    "\n",
    "# Define API endpoint\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    start_time = time.time()\n",
    "    data = request.get_json()\n",
    "    text = data.get('text', '')\n",
    "    if not text:\n",
    "        return jsonify({'error': 'No text provided'}), 400\n",
    "    \n",
    "    # Run prediction\n",
    "    result = sentiment_pipeline(text)[0]\n",
    "    label = result['label']\n",
    "    score = result['score']\n",
    "    \n",
    "    # Mock accuracy (assume positive if score > 0.5, else negative)\n",
    "    mock_true_label = 'POSITIVE' if 'great' in text.lower() else 'NEGATIVE'\n",
    "    accuracy = 1 if label == mock_true_label else 0\n",
    "    \n",
    "    # Record metrics\n",
    "    latency = time.time() - start_time\n",
    "    request_times.append(latency)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    return jsonify({'label': label, 'score': score, 'latency': latency})\n",
    "\n",
    "# Simulate requests (run separately in a terminal or another cell)\n",
    "sample_texts = [\n",
    "    'This product is great and works perfectly!',\n",
    "    'The service was terrible and slow.',\n",
    "    'Amazing experience, highly recommend!'\n",
    "]\n",
    "\n",
    "# Function to test endpoint\n",
    "def test_endpoint():\n",
    "    import requests\n",
    "    for text in sample_texts:\n",
    "        response = requests.post('http://127.0.0.1:5000/predict', json={'text': text})\n",
    "        print(f'Text: {text}')\n",
    "        print(f'Response: {response.json()}')\n",
    "    \n",
    "    # Compute mock monitoring metrics\n",
    "    avg_latency = np.mean(request_times) if request_times else 0\n",
    "    avg_accuracy = np.mean(accuracies) if accuracies else 0\n",
    "    print('\\nMonitoring Metrics:')\n",
    "    print(f'Average Latency: {avg_latency:.4f} seconds')\n",
    "    print(f'Average Accuracy: {avg_accuracy:.4f}')\n",
    "    print('Analysis: [Evaluate latency, accuracy, reliability]')\n",
    "\n",
    "# Run Flask app (run in a separate terminal or background process)\n",
    "# Uncomment to run: app.run(debug=True)\n",
    "\n",
    "# Note: To test, run the Flask app in a terminal (`python -m flask run`)\n",
    "# then execute the test_endpoint() function in another cell or script.\n",
    "# For Colab, use local simulation below:\n",
    "\n",
    "# Local simulation for demo\n",
    "for text in sample_texts:\n",
    "    start_time = time.time()\n",
    "    result = sentiment_pipeline(text)[0]\n",
    "    latency = time.time() - start_time\n",
    "    mock_true_label = 'POSITIVE' if 'great' in text.lower() else 'NEGATIVE'\n",
    "    accuracy = 1 if result['label'] == mock_true_label else 0\n",
    "    request_times.append(latency)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f'Text: {text}')\n",
    "    print(f'Label: {result[\"label\"]}, Score: {result[\"score\"]:.4f}, Latency: {latency:.4f}')\n",
    "\n",
    "# Compute mock monitoring metrics\n",
    "avg_latency = np.mean(request_times) if request_times else 0\n",
    "avg_accuracy = np.mean(accuracies) if accuracies else 0\n",
    "print('\\nMonitoring Metrics:')\n",
    "print(f'Average Latency: {avg_latency:.4f} seconds')\n",
    "print(f'Average Accuracy: {avg_accuracy:.4f}')\n",
    "print('Analysis: [Evaluate latency, accuracy, reliability]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis Questions**:\n",
    "- Is the average latency acceptable for real-time use?\n",
    "- Is the mock accuracy reliable? Why or why not?\n",
    "- How could the deployment or monitoring be improved?\n",
    "\n",
    "**Share**:\n",
    "- Post your metrics, sample predictions, and analysis in the forum.\n",
    "- Example: 'My latency was 0.2s, but accuracy was low. I’ll optimize the model.'\n",
    "\n",
    "**Note**: For full deployment, run the Flask app separately and use the `test_endpoint()` function. The local simulation is for Colab compatibility."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}